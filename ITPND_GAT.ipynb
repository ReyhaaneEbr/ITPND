{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XxRZl6fRAm6I",
        "hj9R3aEyAr2K",
        "6RP7G_m6Aw7N",
        "hu3uhp7LBG0D"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2OWZ6uHGcuD"
      },
      "source": [
        "#GAT_PB IID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxRZl6fRAm6I"
      },
      "source": [
        "##Load Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qgRfm335GmPy"
      },
      "outputs": [],
      "source": [
        "!pip install ogb\n",
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "cwQ2JsjD2kvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJWcu4-8GmPy"
      },
      "outputs": [],
      "source": [
        "from torch._C import *\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj9R3aEyAr2K"
      },
      "source": [
        "##GAT Model Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMZKzLGKGmPz"
      },
      "outputs": [],
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,num_classes,num_layers, dropout):\n",
        "        super(GAT, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.fc = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(GATConv(hidden_channels, out_channels))\n",
        "        self.fc.append(torch.nn.Linear(out_channels,num_classes))\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, adj_t)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        for fc in self.fc[:-1]:\n",
        "            x = fc(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.fc[-1](x)\n",
        "        return x\n",
        "\n",
        "def train(model, data, train_idx, optimizer):\n",
        "      model.train()\n",
        "      criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data.x, data.adj_t)[train_idx]\n",
        "      loss = criterion(out, data.y[train_idx])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      return loss.item()\n",
        "\n",
        "def to_one_hot(y, num_classes):\n",
        "    y_one_hot = torch.zeros(y.size(0), num_classes).to(y.device)\n",
        "    y_one_hot.scatter_(1, y.view(-1, 1), 1)\n",
        "    return y_one_hot\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, num_classes):\n",
        "    model.eval()\n",
        "\n",
        "    y_probs = model(data.x, data.adj_t)\n",
        "    y_probs = torch.softmax(y_probs, dim=1)\n",
        "\n",
        "    y_true_train = to_one_hot(data.y[split_idx['train']], num_classes)\n",
        "    y_true_valid = to_one_hot(data.y[split_idx['valid']], num_classes)\n",
        "    y_true_test = to_one_hot(data.y[split_idx['test']], num_classes)\n",
        "\n",
        "    def compute_metrics(y_true, y_probs):\n",
        "        y_pred_labels = y_probs.argmax(dim=1).cpu().numpy()\n",
        "        y_true_labels = y_true.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(y_true_labels, y_pred_labels)\n",
        "        prec = precision_score(y_true_labels, y_pred_labels, zero_division=0)\n",
        "        rec = recall_score(y_true_labels, y_pred_labels, zero_division=0)\n",
        "        f1 = f1_score(y_true_labels, y_pred_labels, zero_division=0)\n",
        "        rocauc = roc_auc_score(y_true.cpu(), y_probs.cpu())\n",
        "        y_true_cpu = y_true.cpu()\n",
        "        y_true_binarized = label_binarize(y_true_cpu, classes=[0, 1])\n",
        "        y_probs_cpu = y_probs.cpu().numpy()\n",
        "        precision, recall, _ = precision_recall_curve(y_true_binarized[:, 1], [score[1] for score in y_probs_cpu])\n",
        "        auprc = auc(recall, precision)\n",
        "\n",
        "        return [acc, prec, rec, f1, rocauc, auprc]\n",
        "\n",
        "    train_metrics = compute_metrics(y_true_train, y_probs[split_idx['train']])\n",
        "    valid_metrics = compute_metrics(y_true_valid, y_probs[split_idx['valid']])\n",
        "    test_metrics = compute_metrics(y_true_test, y_probs[split_idx['test']])\n",
        "\n",
        "    return train_metrics, valid_metrics, test_metrics\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_kfold(model, data, split_idx, num_classes):\n",
        "    model.eval()\n",
        "\n",
        "    y_probs = model(data.x, data.adj_t)\n",
        "    y_probs = torch.softmax(y_probs, dim=1)\n",
        "\n",
        "    y_true_train = to_one_hot(data.y[split_idx['train']], num_classes)\n",
        "    #y_true_valid = to_one_hot(data.y[split_idx['valid']], num_classes)\n",
        "    y_true_test = to_one_hot(data.y[split_idx['test']], num_classes)\n",
        "\n",
        "    def compute_metrics(y_true, y_probs):\n",
        "        y_pred_labels = y_probs.argmax(dim=1).cpu().numpy()\n",
        "        y_true_labels = y_true.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(y_true_labels, y_pred_labels)\n",
        "        prec = precision_score(y_true_labels, y_pred_labels, zero_division=0)\n",
        "        rec = recall_score(y_true_labels, y_pred_labels, zero_division=0)\n",
        "        f1 = f1_score(y_true_labels, y_pred_labels, zero_division=0)\n",
        "        rocauc = roc_auc_score(y_true.cpu(), y_probs.cpu())\n",
        "        y_true_cpu = y_true.cpu()\n",
        "        y_true_binarized = label_binarize(y_true_cpu, classes=[0, 1])\n",
        "        y_probs_cpu = y_probs.cpu().numpy()\n",
        "        precision, recall, _ = precision_recall_curve(y_true_binarized[:, 1], [score[1] for score in y_probs_cpu])\n",
        "        auprc = auc(recall, precision)\n",
        "\n",
        "        return [acc, prec, rec, f1, rocauc, auprc]\n",
        "\n",
        "    train_metrics = compute_metrics(y_true_train, y_probs[split_idx['train']])\n",
        "    test_metrics = compute_metrics(y_true_test, y_probs[split_idx['test']])\n",
        "\n",
        "    return train_metrics, test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RP7G_m6Aw7N"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNorN9KDBlBH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdDDPUesGmPz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_features = pd.read_csv('/content/gdrive/MyDrive/New_Repo_Data/embeddings_mean_protbert_3k.csv')\n",
        "df_features.iloc[:,1:-1] = np.random.uniform(low=-3, high=6, size=(5609, 1024))\n",
        "\n",
        "df_labels = df_features .iloc[:4819, [0, -1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OAzHFjwGmPz"
      },
      "outputs": [],
      "source": [
        "df_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrgkBkPsTeur"
      },
      "outputs": [],
      "source": [
        "df_labels['Id'] = df_labels['Id'].astype(str)\n",
        "df_features['Id'] = df_features['Id'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApYlpYtQGmP0"
      },
      "outputs": [],
      "source": [
        "order_dict = {value: index for index, value in enumerate(list(df_labels['Id']))}\n",
        "\n",
        "def sorting_key(value):\n",
        "    return (order_dict.get(value, float('inf')), value)\n",
        "\n",
        "df_features = df_features.sort_values(by='Id', key=lambda x: x.map(sorting_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6UkQIadGmP0"
      },
      "outputs": [],
      "source": [
        "df_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6rd6bPeLGmP0"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "graph_data = pd.read_csv('/content/gdrive/MyDrive/New_Repo_Data/new_ppi_edges_iid.csv')\n",
        "id_list = list(df_features['Id'])\n",
        "\n",
        "G = nx.Graph()\n",
        "for id1,id2 in zip(list(graph_data.iloc[:,0]),list(graph_data.iloc[:,1])):\n",
        "  if((str(id1) in id_list) and (str(id2) in id_list)):\n",
        "    G.add_edge(id1,id2)\n",
        "df_features = df_features[df_features['Id'].isin(list(G.nodes()))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuL8Jgx0GmP1"
      },
      "outputs": [],
      "source": [
        "adj_sparse = nx.adjacency_matrix(G, nodelist=list(df_features.iloc[:,0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m77rSCDpGmP1"
      },
      "outputs": [],
      "source": [
        "node_labels = np.array(df_features.iloc[:,-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4xaake9Tjdc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "adj_matrix = coo_matrix(adj_sparse).todense()\n",
        "# Define the number of nodes and features\n",
        "num_nodes = 4819\n",
        "num_features = 1024\n",
        "num_classes = 2\n",
        "\n",
        "node_features = np.array(df_features.iloc[:,1:-1])\n",
        "node_names = np.array(df_features.iloc[:,0])\n",
        "\n",
        "adj_matrix = (adj_matrix + adj_matrix.T) / 2\n",
        "adj_matrix[adj_matrix < 0.9] = 0  # Sparsify the adjacency matrix\n",
        "\n",
        "node_features_tensor = torch.from_numpy(node_features).float()\n",
        "node_labels_tensor = torch.from_numpy(node_labels).long()  # Convert labels to long type\n",
        "\n",
        "adj_coo = adj_sparse.tocoo()\n",
        "indices = np.vstack((adj_coo.row, adj_coo.col))\n",
        "values = adj_sparse.data\n",
        "\n",
        "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
        "values_tensor = torch.tensor(values, dtype=torch.float)\n",
        "\n",
        "adj_t = torch.sparse_coo_tensor(indices_tensor, values_tensor, adj_sparse.shape).to_sparse_csr()\n",
        "\n",
        "data = Data(uni_id=node_names ,x=node_features_tensor, adj_t=adj_t, y=node_labels_tensor)\n",
        "\n",
        "# Assuming your data structure\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Map unique IDs to indices\n",
        "id_to_idx_map = {id_: idx for idx, id_ in enumerate(data.uni_id)}\n",
        "\n",
        "# Function to load folds (train.npy and test.npy) from Google Drive\n",
        "def load_folds(folds_dir):\n",
        "    folds = []\n",
        "    for fold in range(1, 6):  # Assuming 5 folds (1 to 5)\n",
        "        # Define the paths for train.npy and test.npy for each fold\n",
        "        train_path = os.path.join(folds_dir, f'fold_{fold}_train_ids.csv')\n",
        "        test_path = os.path.join(folds_dir, f'fold_{fold}_test_ids.csv')\n",
        "\n",
        "        # Load the train and test files\n",
        "        train_ids = pd.read_csv(train_path)  # This should load the numpy array of IDs\n",
        "        test_ids = pd.read_csv(test_path)\n",
        "\n",
        "        # Map the IDs to indices\n",
        "        train_idx = torch.tensor([id_to_idx_map[id_] for id_ in list (train_ids.iloc[:,0])], dtype=torch.long).to(device)\n",
        "        test_idx = torch.tensor([id_to_idx_map[id_] for id_ in list (test_ids.iloc[:,0])], dtype=torch.long).to(device)\n",
        "\n",
        "        # Append the tuple of train and test indices for this fold\n",
        "        folds.append((train_idx, test_idx))\n",
        "\n",
        "    return folds\n",
        "\n",
        "folds_dir = \"/content/gdrive/MyDrive/New_Repo_Data/5folds\"  # Update this path to your Google Drive directory\n",
        "folds = load_folds(folds_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGAukhwOtjV5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyper-parameters and settings\n",
        "hidden_channels = 128\n",
        "out_channels = 64\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "runs = 5\n",
        "lr = 0.001\n",
        "epochs = 500\n",
        "eval_steps = 10\n",
        "log_steps = 10\n",
        "weights = torch.tensor([2.0,1.0])\n",
        "num_classes = 2\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data and folds\n",
        "data = data.to(device)\n",
        "weights = weights.to(device)\n",
        "model = GAT(in_channels=data.num_features,\n",
        "            hidden_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            num_classes=num_classes,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout).to(device)\n",
        "\n",
        "# Metrics storage\n",
        "train_acc_list, train_prec_list, train_rec_list = [], [], []\n",
        "train_f1_list, train_rocauc_list, train_aucprc_list = [], [], []\n",
        "test_acc_list, test_prec_list, test_rec_list = [], [], []\n",
        "test_f1_list, test_rocauc_list, test_aucprc_list = [], [], []\n",
        "\n",
        "# Main training loop\n",
        "for fold, (train_idx, test_idx) in enumerate(folds):\n",
        "    print(f'\\n=== Fold {fold + 1}/{len(folds)} ===')\n",
        "    train_idx = torch.tensor(train_idx, dtype=torch.long).to(device)\n",
        "    test_idx = torch.tensor(test_idx, dtype=torch.long).to(device)\n",
        "    split_idx = {'train': train_idx, 'test': test_idx}\n",
        "\n",
        "    model.reset_parameters()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_train_rocauc = 0\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss = train(model, data, train_idx, optimizer)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Early stopping check\n",
        "        if len(loss_history) > 10 and np.std(loss_history[-10:]) < 1e-3:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % eval_steps == 0 or epoch == epochs:\n",
        "            train_metrics, test_metrics = test_kfold(model, data, split_idx, num_classes)\n",
        "            train_rocauc = train_metrics[4]\n",
        "\n",
        "            # Save best model\n",
        "            if train_rocauc > best_train_rocauc:\n",
        "                best_train_rocauc = train_rocauc\n",
        "                torch.save(model.state_dict(), f'best_model_fold_{fold}.pt')\n",
        "\n",
        "    # Final evaluation with best model\n",
        "    model.load_state_dict(torch.load(f'best_model_fold_{fold}.pt'))\n",
        "    train_metrics, test_metrics = test_kfold(model, data, split_idx, num_classes)\n",
        "\n",
        "    # Store metrics\n",
        "    for lst, values in zip([train_acc_list, train_prec_list, train_rec_list, train_f1_list, train_rocauc_list, train_aucprc_list],\n",
        "                           train_metrics):\n",
        "        lst.append(values)\n",
        "\n",
        "    for lst, values in zip([test_acc_list, test_prec_list, test_rec_list, test_f1_list, test_rocauc_list, test_aucprc_list],\n",
        "                           test_metrics):\n",
        "        lst.append(values)\n",
        "\n",
        "# Metrics calculations\n",
        "def calculate_stats(metric_list, name):\n",
        "    return {\n",
        "        'mean': np.mean(metric_list),\n",
        "        'std': np.std(metric_list),\n",
        "        'var': np.var(metric_list),\n",
        "        'min': np.min(metric_list),\n",
        "        'max': np.max(metric_list)\n",
        "    }\n",
        "\n",
        "# Generate report\n",
        "print(\"\\n=== Final Report ===\")\n",
        "print(\"{:<15} {:<8} {:<8} {:<8} {:<8} {:<8}\".format(\n",
        "    'Metric', 'Mean', 'Std', 'Var', 'Min', 'Max'))\n",
        "\n",
        "for metric_name, train_list, test_list in [\n",
        "    ('Accuracy', train_acc_list, test_acc_list),\n",
        "    ('Precision', train_prec_list, test_prec_list),\n",
        "    ('Recall', train_rec_list, test_rec_list),\n",
        "    ('F1', train_f1_list, test_f1_list),\n",
        "    ('ROC AUC', train_rocauc_list, test_rocauc_list),\n",
        "    ('PR AUC', train_aucprc_list, test_aucprc_list)\n",
        "]:\n",
        "    train_stats = calculate_stats(train_list, 'Train')\n",
        "    test_stats = calculate_stats(test_list, 'Test')\n",
        "\n",
        "    print(f\"\\n**{metric_name}**\")\n",
        "    print(\"Train:\\t{mean:.4f} ± {std:.4f}\\t(var: {var:.4f})\\t[{min:.4f}-{max:.4f}]\".format(**train_stats))\n",
        "    print(\"Test:\\t{mean:.4f} ± {std:.4f}\\t(var: {var:.4f})\\t[{min:.4f}-{max:.4f}]\".format(**test_stats))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu3uhp7LBG0D"
      },
      "source": [
        "##Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11aKefLAB1FN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Hyper-parameters and settings\n",
        "hidden_channels = 128\n",
        "out_channels = 64\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "runs = 5\n",
        "lr = 0.001\n",
        "epochs = 500\n",
        "eval_steps = 10\n",
        "log_steps = 10\n",
        "weights = torch.tensor([2.0,1.0])\n",
        "num_classes = 2\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data and folds\n",
        "data = data.to(device)\n",
        "weights = weights.to(device)\n",
        "model = GAT(in_channels=data.num_features,\n",
        "            hidden_channels=hidden_channels,\n",
        "            out_channels=out_channels,\n",
        "            num_classes=num_classes,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout).to(device)\n",
        "\n",
        "# Metrics storage\n",
        "train_acc_list, train_prec_list, train_rec_list = [], [], []\n",
        "train_f1_list, train_rocauc_list, train_aucprc_list = [], [], []\n",
        "test_acc_list, test_prec_list, test_rec_list = [], [], []\n",
        "test_f1_list, test_rocauc_list, test_aucprc_list = [], [], []\n",
        "\n",
        "# Main training loop\n",
        "for fold, (train_idx, test_idx) in enumerate(folds):\n",
        "    print(f'\\n=== Fold {fold + 1}/{len(folds)} ===')\n",
        "    train_idx = torch.tensor(train_idx, dtype=torch.long).to(device)\n",
        "    test_idx = torch.tensor(test_idx, dtype=torch.long).to(device)\n",
        "    split_idx = {'train': train_idx, 'test': test_idx}\n",
        "\n",
        "    model.reset_parameters()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_train_rocauc = 0\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss = train(model, data, train_idx, optimizer)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Early stopping check\n",
        "        if len(loss_history) > 10 and np.std(loss_history[-10:]) < 1e-3:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % eval_steps == 0 or epoch == epochs:\n",
        "            train_metrics, test_metrics = test_kfold(model, data, split_idx, num_classes)\n",
        "            train_rocauc = train_metrics[4]\n",
        "\n",
        "            # Save best model\n",
        "            if train_rocauc > best_train_rocauc:\n",
        "                best_train_rocauc = train_rocauc\n",
        "                torch.save(model.state_dict(), f'best_model_fold_{fold}.pt')\n",
        "\n",
        "    # Final evaluation with best model\n",
        "    model.load_state_dict(torch.load(f'best_model_fold_{fold}.pt'))\n",
        "    train_metrics, test_metrics = test_kfold(model, data, split_idx, num_classes)\n",
        "\n",
        "    # Store metrics\n",
        "    for lst, values in zip([train_acc_list, train_prec_list, train_rec_list, train_f1_list, train_rocauc_list, train_aucprc_list],\n",
        "                           train_metrics):\n",
        "        lst.append(values)\n",
        "\n",
        "    for lst, values in zip([test_acc_list, test_prec_list, test_rec_list, test_f1_list, test_rocauc_list, test_aucprc_list],\n",
        "                           test_metrics):\n",
        "        lst.append(values)\n",
        "\n",
        "# Metrics calculations\n",
        "def calculate_stats(metric_list, name):\n",
        "    return {\n",
        "        'mean': np.mean(metric_list),\n",
        "        'std': np.std(metric_list),\n",
        "        'var': np.var(metric_list),\n",
        "        'min': np.min(metric_list),\n",
        "        'max': np.max(metric_list)\n",
        "    }\n",
        "\n",
        "# Generate report\n",
        "print(\"\\n=== Final Report ===\")\n",
        "print(\"{:<15} {:<8} {:<8} {:<8} {:<8} {:<8}\".format(\n",
        "    'Metric', 'Mean', 'Std', 'Var', 'Min', 'Max'))\n",
        "\n",
        "for metric_name, train_list, test_list in [\n",
        "    ('Accuracy', train_acc_list, test_acc_list),\n",
        "    ('Precision', train_prec_list, test_prec_list),\n",
        "    ('Recall', train_rec_list, test_rec_list),\n",
        "    ('F1', train_f1_list, test_f1_list),\n",
        "    ('ROC AUC', train_rocauc_list, test_rocauc_list),\n",
        "    ('PR AUC', train_aucprc_list, test_aucprc_list)\n",
        "]:\n",
        "    train_stats = calculate_stats(train_list, 'Train')\n",
        "    test_stats = calculate_stats(test_list, 'Test')\n",
        "\n",
        "    print(f\"\\n**{metric_name}**\")\n",
        "    print(\"Train:\\t{mean:.4f} ± {std:.4f}\\t(var: {var:.4f})\\t[{min:.4f}-{max:.4f}]\".format(**train_stats))\n",
        "    print(\"Test:\\t{mean:.4f} ± {std:.4f}\\t(var: {var:.4f})\\t[{min:.4f}-{max:.4f}]\".format(**test_stats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5axtGzhB1FO"
      },
      "outputs": [],
      "source": []
    }
  ]
}